{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd0zXyOxwyaT"
      },
      "source": [
        "# Proses Analisis Sentimen Pada Ulasan Aplikasi Peduli Lindungi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r_ynkDUx3O0"
      },
      "source": [
        "### Menginstall *package* yang dibutuhkan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJD4K562MKNF",
        "outputId": "66e37f07-c388-4fc9-a290-04e7d04f2347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping googletrans as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.12.7)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16352 sha256=22871f3376a3959ce968b6a445cb2ffd67347f445b7c21a11f228668fa59a46f\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 uninstall googletrans\n",
        "!pip3 install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vudjb1WFw8ws"
      },
      "source": [
        "### Mengimpor *library* yang dibutuhkan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ0eKL93xgot",
        "outputId": "968f39ca-daa2-42b8-bb03-f7298d9c7baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from googletrans import Translator\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Filtering\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stemming\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Labeling\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, auc, roc_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyrO9GlELm_P"
      },
      "source": [
        "###  Mengimpor dan menampilkan data dalam bentuk tabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "SpBvkOznBtlq",
        "outputId": "6e14e50d-b73d-47c0-b6e0-7c71110b8d3f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-494cc47ff16d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Scrapped_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Scrapped_data.csv'"
          ]
        }
      ],
      "source": [
        "path = 'Scrapped_data.csv'\n",
        "data = pd.read_csv(path)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5827)"
      ],
      "metadata": {
        "id": "-TMdlFyn2vj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDH1GOHeL733"
      },
      "source": [
        "### Menampilkan jumlah baris dan kolom data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-SPjDrjB6nx"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MYZ077xMIAC"
      },
      "source": [
        "### Menghilangkan variabel yang tidak dipakai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdD6FlmqB6lZ"
      },
      "outputs": [],
      "source": [
        "df_data = data.copy()\n",
        "df_data = data.drop(columns = ['Nama reviewer','Rating','Tanggal ulasan'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2xn8coj0jg7"
      },
      "source": [
        "### Menerjemahkan ke dalam bahasa Inggris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_luUjAGZ0ieQ"
      },
      "outputs": [],
      "source": [
        "translator = Translator()\n",
        "df_data['Ulasan'] = df_data['Ulasan'].apply(translator.translate, src='id', dest='en').apply(getattr, args=('text',))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svidGkFPzTiZ"
      },
      "outputs": [],
      "source": [
        "df_data_en = df_data.copy()\n",
        "df_data_en.to_csv('data_en.csv', index=False)\n",
        "df_data_en.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhxHrzb00wax"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrmDz1n7CNF4"
      },
      "source": [
        "### *Data Preprocessing*\n",
        "#### *Case Folding*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQYw-4_tB6hp"
      },
      "outputs": [],
      "source": [
        "def case_folding(content):\n",
        "  content = content.lower()                             # mengecilkan huruf\n",
        "  content = content.strip(' ')                          # menghapus spasi diawal dan diakhir string\n",
        "  content = re.sub(r'[?|$|.,|!_:)(-+)]', '', content)   # mengganti tanda ?|$ dsb dengan kosong\n",
        "  content = re.sub(r'\\d', '', content)                  # menghapus angka\n",
        "  content = re.sub('[^\\w\\s]', '',content)               # menghapus punctuations\n",
        "  content = re.sub('  ', ' ',content)                   # menghapus dua spasi\n",
        "  return content\n",
        "df_data_en['Ulasan'] = df_data_en['Ulasan'].apply(case_folding)\n",
        "df_data_en.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_en.to_csv('data_CF.csv', index=False)"
      ],
      "metadata": {
        "id": "lfXRQ5Y2KzUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BteG6orC89J"
      },
      "source": [
        "#### *Tokenizing*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNhS4N4sCVIR"
      },
      "outputs": [],
      "source": [
        "def token(content):\n",
        "  nstr = content.split(' ')\n",
        "  dat = []\n",
        "  a = -1\n",
        "  for hu in nstr:\n",
        "    a = a+1\n",
        "  if hu == '':\n",
        "    dat.append(a)\n",
        "  return nstr\n",
        "df_data_en['Ulasan'] = df_data_en['Ulasan'].apply(token)\n",
        "df_data_en.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_en.to_csv('data_Toke.csv', index=False)"
      ],
      "metadata": {
        "id": "XRb66HF7K_wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVvPOEsoDNZo"
      },
      "source": [
        "#### *Filtering*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNhcgFXHCVFC"
      },
      "outputs": [],
      "source": [
        "def stopwords_removal(content):\n",
        "  filtering = stopwords.words('english')\n",
        "  x = []\n",
        "  data = []\n",
        "  def myFunc(x):\n",
        "    if x in filtering:\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "  fit = filter(myFunc, content)\n",
        "  for x in fit:\n",
        "    data.append(x)\n",
        "  return data\n",
        "df_data_en['Ulasan'] = df_data_en['Ulasan'].apply(stopwords_removal)\n",
        "df_data_en.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_en.to_csv('data_Filter.csv', index=False)"
      ],
      "metadata": {
        "id": "cpcE9waXLJ1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBaDBnXiDzLb"
      },
      "source": [
        "#### *Stemming*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLKHtwPtCVBF"
      },
      "outputs": [],
      "source": [
        "def stemming(content):\n",
        "  stemmer = nltk.porter.PorterStemmer()\n",
        "  do = []\n",
        "  for w in content:\n",
        "    dt = stemmer.stem(w)\n",
        "    do.append(dt)\n",
        "  d_clean = []\n",
        "  d_clean = \" \".join(do)\n",
        "  return d_clean\n",
        "df_data_en['Ulasan'] = df_data_en['Ulasan'].apply(stemming)\n",
        "\n",
        "df_data_en.to_csv('data_cleans.csv', index=False)\n",
        "data_cleans = pd.read_csv('data_cleans.csv', encoding='latin1')\n",
        "data_cleans.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_en.to_csv('data_Stem.csv', index=False)"
      ],
      "metadata": {
        "id": "6-2jOCcxLUAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grz7Ur4GeWLn"
      },
      "source": [
        "#### Menampilkan hasil *data preprocessing*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1YOHBrWFqMX"
      },
      "outputs": [],
      "source": [
        "data_clean = data_cleans.dropna()\n",
        "data_clean.to_csv('data_clean.csv', index=False)\n",
        "data_cleans = pd.read_csv('data_clean.csv', encoding='latin1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw--KG1Mk2Ky"
      },
      "outputs": [],
      "source": [
        "data_cleans = pd.read_csv('data_clean.csv', encoding='latin1')\n",
        "data_cleans.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXZKvNVq6EjP"
      },
      "outputs": [],
      "source": [
        "data_cleans.sample(6, random_state = 134)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da0Rbw12eyCp"
      },
      "source": [
        "#### Mengecek data *missing value*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un_JkLzsFXHB"
      },
      "outputs": [],
      "source": [
        "data_cleans.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjpt1lgBfHdk"
      },
      "outputs": [],
      "source": [
        "len(data_cleans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APWaS40KFgcY"
      },
      "source": [
        "#### Melakukan pelabelan pada data ulasan aplikasi Peduli Lindungi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O840dE7mFXEv"
      },
      "outputs": [],
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "sia.polarity_scores(\"The film was awesome\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icETAGa0I46w"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores(\"I liked this music but it is not good as the other one\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naNCLpjcJAe0"
      },
      "outputs": [],
      "source": [
        "data_cleans[\"Ulasan\"][0:10].apply(lambda x: sia.polarity_scores(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEvdqM5iFW8k"
      },
      "outputs": [],
      "source": [
        "data_cleans['Ulasan'][0:10].apply(lambda x: sia.polarity_scores(x)[\"compound\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o_SO89BIexd"
      },
      "outputs": [],
      "source": [
        "data_cleans[\"polarity_score\"] = data_cleans['Ulasan'].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
        "data_cleans.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3OrqPmyKr7U"
      },
      "outputs": [],
      "source": [
        "data_cleans[\"Ulasan\"][0:10].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAE2YK7Co7FD"
      },
      "outputs": [],
      "source": [
        "data_cleans[\"sentiment_label\"] = data_cleans[\"Ulasan\"].apply(lambda x: 1 if sia.polarity_scores(x)[\"compound\"] > 0 else 0)\n",
        "data_cleans.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVFmVyUCpB8W"
      },
      "outputs": [],
      "source": [
        "data_cleans[\"sentiment_label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uev5kjMapB5D"
      },
      "outputs": [],
      "source": [
        "data_cleans = data_cleans.astype({'sentiment_label': 'category'})\n",
        "data_cleans = data_cleans.astype({'Ulasan': 'string'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3azAMadZ6FwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmeVQFpXpe4m"
      },
      "source": [
        "#### Melakukan pembobotan *TF-IDF*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0f7f9uepB1E"
      },
      "outputs": [],
      "source": [
        "tf = TfidfVectorizer()\n",
        "text_tf = tf.fit_transform(data_cleans['Ulasan'].astype('U'))\n",
        "text_tf[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZlmYI64UrnW"
      },
      "outputs": [],
      "source": [
        "tf_count = data_cleans['Ulasan'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n",
        "\n",
        "tf_count.columns = [\"words\", \"tf\"]\n",
        "tf_count.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEkHLn5RWSZS"
      },
      "outputs": [],
      "source": [
        "tf_count[(tf_count['words'] == \"drain\") | (tf_count['words'] == \"batteri\") | (tf_count['words'] == \"good\") | (tf_count['words'] == \"better\") | (tf_count['words'] == \"last\") | (tf_count['words'] == \"year\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zmaYZjapoNG"
      },
      "source": [
        "#### Membagi Data (*data training* sebanyak 80 persen dan *data testing* sebanyak 20 persen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3QjFr-qpBwo"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(text_tf, data_cleans['sentiment_label'], test_size=0.2, random_state=42)\n",
        "# x1_train, x1_test, y1_train, y1_test = train_test_split(text_tf, data_cleans['sentiment_label'], test_size=0.25, random_state=42)\n",
        "# x2_train, x2_test, y2_train, y2_test = train_test_split(text_tf, data_cleans['sentiment_label'], test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1_train, x1_test, y1_train, y1_test = train_test_split(data_cleans['Ulasan'], data_cleans['sentiment_label'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zPaZmFO66ohJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1_test.sample(1)"
      ],
      "metadata": {
        "id": "IEZdXQcJ7j6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1_test[5274], y1_test[5274]"
      ],
      "metadata": {
        "id": "8jwwf52R6xhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sentimen_testing = y1_test.to_frame().reset_index(False)\n",
        "testing_pos = Sentimen_testing[Sentimen_testing['sentiment_label']==1]\n",
        "testing_neg = Sentimen_testing[Sentimen_testing['sentiment_label']==0]"
      ],
      "metadata": {
        "id": "6HbX6SkI9lg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_test_pos = []\n",
        "index_test_neg = []\n",
        "for i in testing_pos['index']:\n",
        "  index_test_pos.append(i)\n",
        "for i in testing_neg['index']:\n",
        "  index_test_neg.append(i)"
      ],
      "metadata": {
        "id": "EG8a6_iFDi9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos = pd.DataFrame()\n",
        "for i in index_test_pos:\n",
        "  test_pos.append(data_cleans['Ulasan'].iloc[i])\n",
        "\n",
        "test_pos"
      ],
      "metadata": {
        "id": "0T-8z-uqE8I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlVP8da-lIog"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.value_counts()"
      ],
      "metadata": {
        "id": "EFmfGSPF6fln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJukdFK__wXd"
      },
      "source": [
        "#### Proses ADASYN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUqbEhM6_3Jx"
      },
      "outputs": [],
      "source": [
        "data_ada = data_cleans.sample(10, random_state = 46).reset_index().drop(\"index\", axis=1)\n",
        "data_ada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3uFW__GA4rF"
      },
      "outputs": [],
      "source": [
        "data_ada['sentiment_label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "506Hr0ERBUZm"
      },
      "outputs": [],
      "source": [
        "tf = TfidfVectorizer()\n",
        "text_tf_ada = tf.fit_transform(data_ada['Ulasan'].astype('U'))\n",
        "text_tf_ada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHUubXSPPfQh"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "k = 5\n",
        "\n",
        "# Build K Nearest Neighbors model\n",
        "knn_model = NearestNeighbors(n_neighbors=k, metric=\"euclidean\").fit(text_tf_ada)\n",
        "distances, indices = knn_model.kneighbors()\n",
        "\n",
        "\n",
        "# Print the 'k' nearest neighbors\n",
        "print(\"K Nearest Neighbors dg pusat D1:\")\n",
        "for rank, index in enumerate(indices[1][:k], start=1):\n",
        "    print(str(rank) + \" ==>\", index)\n",
        "print(\"K Nearest Neighbors dg pusat D6\")\n",
        "for rank, index in enumerate(indices[6][:k], start=1):\n",
        "    print(str(rank) + \" ==>\", index)\n",
        "print(\"K Nearest Neighbors dg pusat D9\")\n",
        "for rank, index in enumerate(indices[9][:k], start=1):\n",
        "    print(str(rank) + \" ==>\", index)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYP9PU0IrUIC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "a = [0, 8, 7, 4, 2]\n",
        "b = [4, 8, 7, 0, 5]\n",
        "c = [7, 8, 4, 2, 6]\n",
        "print(\"Jarak euclidean K Nearest Neighbors dg pusat D0:\")\n",
        "for i in a:\n",
        "  print(str(i), \"==>\", euclidean_distances(text_tf_ada[0],text_tf_ada[i]))\n",
        "print(\"Jarak euclidean K Nearest Neighbors dg pusat D1:\")\n",
        "for i in b:\n",
        "  print(str(i), \"==>\", euclidean_distances(text_tf_ada[6],text_tf_ada[i]))\n",
        "print(\"Jarak euclidean K Nearest Neighbors dg pusat D2:\")\n",
        "for i in c:\n",
        "  print(str(i), \"==>\", euclidean_distances(text_tf_ada[9],text_tf_ada[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diTpBqD7p9T6"
      },
      "source": [
        "## Algoritma Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simulasi MNB"
      ],
      "metadata": {
        "id": "1i7eP2qenHuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data training\n",
        "data_cleans.sample(30, random_state =134).drop('polarity_score', axis = 1)"
      ],
      "metadata": {
        "id": "sDg0T7AOnG81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAgcVVSetbW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data testing\n",
        "data_cleans.sample(1, random_state = 300).drop('polarity_score', axis = 1)"
      ],
      "metadata": {
        "id": "pq7Kemqdnokc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK3NHwecqAsI"
      },
      "source": [
        "### Imbalanced Data Modelling\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX6dhGOgp_33"
      },
      "outputs": [],
      "source": [
        "clf = MultinomialNB().fit(x_train,y_train)\n",
        "predicted = clf.predict(x_test)\n",
        "false_positive_rate, true_positive_rate, tresholds = roc_curve(y_test,clf.predict(x_test))\n",
        "print('MultinomialNB Accuracy: ', accuracy_score(y_test,predicted))\n",
        "print('MultinomialNB Precision: ', precision_score(y_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB Recall: ', recall_score(y_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB f1-score: ', f1_score(y_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB AUC: ', auc(false_positive_rate, true_positive_rate))\n",
        "\n",
        "print(f'confusion matrix:\\n {confusion_matrix(y_test,predicted)}')\n",
        "print('======================================\\n')\n",
        "print(classification_report(y_test,predicted,zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pju_xzuvqVXp"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
        "\n",
        "clf = MultinomialNB().fit(x1_train,y1_train)\n",
        "predicted = clf.predict(x1_test)\n",
        "false_positive_rate, true_positive_rate, tresholds = roc_curve(y1_test,clf.predict_proba(x1_test)[:,1])\n",
        "print('MultinomialNB Accuracy: ', accuracy_score(y1_test,predicted))\n",
        "print('MultinomialNB Precision: ', precision_score(y1_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB Recall: ', recall_score(y1_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB f1-score: ', f1_score(y1_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB AUC: ', auc(false_positive_rate, true_positive_rate))\n",
        "\n",
        "print(f'confusion matrix:\\n {confusion_matrix(y1_test,predicted)}')\n",
        "print('======================================\\n')\n",
        "print(classification_report(y1_test,predicted,zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10ZNqRqYqVUa"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
        "\n",
        "clf = MultinomialNB().fit(x2_train,y2_train)\n",
        "predicted = clf.predict(x2_test)\n",
        "false_positive_rate, true_positive_rate, tresholds = roc_curve(y2_test,clf.predict_proba(x2_test)[:,1])\n",
        "print('MultinomialNB Accuracy: ', accuracy_score(y2_test,predicted))\n",
        "print('MultinomialNB Precision: ', precision_score(y2_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB Recall: ', recall_score(y2_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB f1-score: ', f1_score(y2_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB AUC: ', auc(false_positive_rate, true_positive_rate))\n",
        "\n",
        "\n",
        "\n",
        "print(f'confusion matrix:\\n {confusion_matrix(y2_test,predicted)}')\n",
        "print('======================================\\n')\n",
        "print(classification_report(y2_test,predicted,zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuH_erqQsUZZ"
      },
      "source": [
        "### SMOTE Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR3KoaxYke_p"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "x = x_train\n",
        "y = y_train\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train.value_counts())\n",
        "\n",
        "smote = SMOTE(sampling_strategy= 'minority')\n",
        "x_sm, y_sm =smote.fit_resample(x_train, y_train)\n",
        "\n",
        "print(y_sm.value_counts())\n",
        "print(y_test.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_qLotHXsqjj"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
        "\n",
        "clf = MultinomialNB().fit(x_sm,y_sm)\n",
        "predicted = clf.predict(x_test)\n",
        "false_positive_rate, true_positive_rate, tresholds = roc_curve(y_test, clf.predict(x_test))\n",
        "print('MultinomialNB Accuracy: ', accuracy_score(y_test, predicted))\n",
        "print('MultinomialNB Precision: ', precision_score(y_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB Recall: ', recall_score(y_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB f1-score: ', f1_score(y_test,predicted, average='binary', pos_label=0))\n",
        "#print('MultinomialNB AUC: ', roc_auc_score(y_test,clf.predict_proba(x_test)[:,1]))\n",
        "print('MultinomialNB AUC: ', auc(false_positive_rate, true_positive_rate))\n",
        "\n",
        "print(f'confusion matrix:\\n {confusion_matrix(y_test,predicted)}')\n",
        "print('======================================\\n')\n",
        "print(classification_report(y_test,predicted,zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALzsNSLBV8hr"
      },
      "outputs": [],
      "source": [
        "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, predicted).ravel()\n",
        "print(\"True Negatives: \",tn)\n",
        "print(\"False Positives: \",fp)\n",
        "print(\"False Negatives: \",fn)\n",
        "print(\"True Positives: \",tp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzsYHTp9Bilw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(y_test,predicted)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7S7ylnFpP39"
      },
      "source": [
        "## ADASYN Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLJHP-vcpaxQ"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "x = x_train\n",
        "y = y_train\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(y.value_counts())\n",
        "\n",
        "ada = ADASYN(sampling_strategy= 'minority')\n",
        "x_ada, y_ada =ada.fit_resample(x, y)\n",
        "\n",
        "print(y_ada.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yLDkmfhpao0"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score,auc,roc_curve\n",
        "\n",
        "clf = MultinomialNB().fit(x_ada,y_ada)\n",
        "predicted = clf.predict(x_test)\n",
        "false_positive_rate, true_positive_rate, tresholds = roc_curve(y_test,clf.predict(x_test))\n",
        "print('MultinomialNB Accuracy: ', accuracy_score(y_test,predicted))\n",
        "print('MultinomialNB Precision: ', precision_score(y_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB Recall: ', recall_score(y_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB f1-score: ', f1_score(y_test,predicted, average='binary', pos_label=0))\n",
        "print('MultinomialNB AUC: ', auc(false_positive_rate, true_positive_rate))\n",
        "\n",
        "print(f'confusion matrix:\\n {confusion_matrix(y_test,predicted)}')\n",
        "print('======================================\\n')\n",
        "print(classification_report(y_test,predicted,zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhFhPRnqBNmZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(y_test,predicted)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
        "tn, fp, fn, tp = metrics.confusion_matrix(y_test, predicted).ravel()\n",
        "print(\"True Negatives: \",tn)\n",
        "print(\"False Positives: \",fp)\n",
        "print(\"False Negatives: \",fn)\n",
        "print(\"True Positives: \",tp)"
      ],
      "metadata": {
        "id": "dc3aQteoIVUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzSMRC3o2vy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data_cleans[\"Ulasan\"], data_cleans['sentiment_label'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KpfxwjXZ2vkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write  predictions to file\n",
        "x_test = pd.DataFrame(x_test).reset_index().drop('index', axis = 1)\n",
        "test_y = pd.DataFrame(y_test).reset_index().drop('index', axis = 1) #Actual values\n",
        "yhat = pd.DataFrame(predicted) #predicted values using the model\n",
        "\n",
        "# x_test.rename(columns= {0: \"Test Data\"}, inplace = True)\n",
        "test_y.rename(columns= {0: 'Label Sentimen'}, inplace = True)\n",
        "yhat.rename(columns = {0: 'Hasil Prediksi'}, inplace = True)\n",
        "\n",
        "new = pd.concat([x_test, test_y, yhat], axis = 1)\n",
        "#print(('ct'+ str(i)))\n",
        "new.to_excel('Hasil_testing.xlsx', index = False)"
      ],
      "metadata": {
        "id": "Q0cX74uFKVwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.head()"
      ],
      "metadata": {
        "id": "loPTLlflLXEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHjQYqvLq77G"
      },
      "source": [
        "## SMOTE + Tomek Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZ5Y1adFreVS"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf = TfidfVectorizer()\n",
        "text_tf5 = tf.fit_transform(data_cleans['content'].astype('U'))\n",
        "text_tf5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JznmJCyureRK"
      },
      "outputs": [],
      "source": [
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "x = text_tf5\n",
        "y = data_cleans['sentiment_label']\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(y.value_counts())\n",
        "\n",
        "smtom = SMOTETomek(sampling_strategy= 'minority')\n",
        "x_smtom, y_smtom =smtom.fit_resample(x, y)\n",
        "\n",
        "print(y_smtom.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYg_C_JOreNq"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "clf = MultinomialNB().fit(x_smtom,y_smtom.ravel())\n",
        "predicted = clf.predict(x_test)\n",
        "print('MultinomialNB Accuracy: ', accuracy_score(y_test,predicted))\n",
        "print('MultinomialNB Precision: ', precision_score(y_test,predicted, average='binary', pos_label=\"neg\"))\n",
        "print('MultinomialNB Recall: ', recall_score(y_test,predicted, average='binary', pos_label=\"neg\"))\n",
        "print('MultinomialNB f1-score: ', f1_score(y_test,predicted, average='binary', pos_label=\"neg\"))\n",
        "print('MultinomialNB AUC: ', roc_auc_score(y_test,clf.predict_proba(x_test)[:,1]))\n",
        "\n",
        "print(f'confusion matrix:\\n {confusion_matrix(y_test,predicted)}')\n",
        "print('======================================\\n')\n",
        "print(classification_report(y_test,predicted,zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO_f_kTrsU1Q"
      },
      "source": [
        "## SMOTE + ENN Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQtcffMcreJW"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf = TfidfVectorizer()\n",
        "text_tf6 = tf.fit_transform(data_cleans['content'].astype('U'))\n",
        "text_tf6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhhgcO_gsiyp"
      },
      "outputs": [],
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "x = text_tf6\n",
        "y = data_cleans['sentiment_label']\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(y.value_counts())\n",
        "\n",
        "smenn = SMOTEENN(sampling_strategy= 'minority')\n",
        "x_smenn, y_smenn =smenn.fit_resample(x, y)\n",
        "\n",
        "print(y_smenn.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgxx5drLsiD7"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "clf = MultinomialNB().fit(x_smenn,y_smenn.ravel())\n",
        "predicted = clf.predict(x_test)\n",
        "print('MultinomialNB Accuracy: ', accuracy_score(y_test,predicted))\n",
        "print('MultinomialNB Precision: ', precision_score(y_test,predicted, average='binary', pos_label=\"neg\"))\n",
        "print('MultinomialNB Recall: ', recall_score(y_test,predicted, average='binary', pos_label=\"neg\"))\n",
        "print('MultinomialNB f1-score: ', f1_score(y_test,predicted, average='binary', pos_label=\"neg\"))\n",
        "print('MultinomialNB AUC: ', roc_auc_score(y_test,clf.predict_proba(x_test)[:,1]))\n",
        "\n",
        "print(f'confusion matrix:\\n {confusion_matrix(y_test,predicted)}')\n",
        "print('======================================\\n')\n",
        "print(classification_report(y_test,predicted,zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52XHPNBgreEV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGageugUtNy9"
      },
      "source": [
        "## Visualisasi "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-oaeVJ0sqXP"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ENj2ehtQYx"
      },
      "outputs": [],
      "source": [
        "data_cleans['sentiment_label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqhOuk6itbH3"
      },
      "outputs": [],
      "source": [
        "data_cleans.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA49V-gEtbET"
      },
      "outputs": [],
      "source": [
        "from textblob import Word, TextBlob\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jofy123LtbAm"
      },
      "outputs": [],
      "source": [
        "sentimen_pos = data_cleans[data_cleans['sentiment_label']==1]\n",
        "sentimen_neg = data_cleans[data_cleans['sentiment_label']==0]\n",
        "sentimen_pos.to_csv('sentimen_positif.csv', index=False)\n",
        "sentimen_neg.to_csv('sentimen_negatif.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hl7RKUutg8-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "visual_pos = pd.read_csv('sentimen_positif.csv', encoding='latin1')\n",
        "visual_pos.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xxN9hC4tg40"
      },
      "outputs": [],
      "source": [
        "visual_neg = pd.read_csv('sentimen_negatif.csv', encoding='latin1')\n",
        "visual_neg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW4eiK_btyn8"
      },
      "outputs": [],
      "source": [
        "tf_pos = visual_pos['Ulasan'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n",
        "\n",
        "tf_pos.columns = [\"words\", \"tf\"]\n",
        "tf_pos.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdJigQXftykj"
      },
      "outputs": [],
      "source": [
        "tf_neg = visual_neg['Ulasan'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n",
        "\n",
        "tf_neg.columns = [\"words\", \"tf\"]\n",
        "tf_neg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8vzzv-lyM3k"
      },
      "outputs": [],
      "source": [
        "tf_pos[tf_pos[\"words\"]==\"good\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZBnbFN_tyg1"
      },
      "outputs": [],
      "source": [
        "tf_pos.shape, tf_neg.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-BgVBeTtybE"
      },
      "outputs": [],
      "source": [
        "tf_pos[\"words\"].nunique(), tf_neg[\"words\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANtoI5-wtyVE"
      },
      "outputs": [],
      "source": [
        "tf_pos[\"tf\"].describe([0.05, 0.10, 0.25, 0.50, 0.75, 0.80, 0.90, 0.95, 0.99]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hM-xrfvuTCG"
      },
      "outputs": [],
      "source": [
        "tf_neg[\"tf\"].describe([0.05, 0.10, 0.25, 0.50, 0.75, 0.80, 0.90, 0.95, 0.99]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwgdivgYuS-r"
      },
      "outputs": [],
      "source": [
        "# Barplot sentimen positif\n",
        "\n",
        "tf_pos[tf_pos[\"tf\"] > 200].plot.bar(x=\"words\", y=\"tf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6tvN7fmuS6k"
      },
      "outputs": [],
      "source": [
        "# Barplot sentimen negatif\n",
        "\n",
        "tf_neg[tf_neg[\"tf\"] > 310].plot.bar(x=\"words\", y=\"tf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef_0X-8nuS27"
      },
      "outputs": [],
      "source": [
        "# Wordcloud sentimen positif\n",
        "\n",
        "text = \" \".join(i for i in visual_pos['Ulasan'])\n",
        "wordcloud = WordCloud(max_font_size=50,\n",
        "                      max_words=200,\n",
        "                      background_color=\"black\",width=250, height=250).generate(text)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('sentimen positif.png', format='png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGGtoP6Cuu4i"
      },
      "outputs": [],
      "source": [
        "# Wordcloud sentimen negatif\n",
        "\n",
        "text = \" \".join(i for i in visual_neg['Ulasan'])\n",
        "wordcloud = WordCloud(max_font_size=50,\n",
        "                      max_words=200,\n",
        "                      background_color=\"black\",width=250, height=250).generate(text)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('sentimen negatif.png', format='png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQUBpt9UUhtf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sHjQYqvLq77G",
        "iO_f_kTrsU1Q"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
